Imports

```{r}
library(fpp2)
library(ggplot2)
```

Load data
```{r}
load("../data/power.rda")
load("../data/temp.rda")
```

## Explore the dataset

```{r}
autoplot(decompose(power, type='additive'))
```


```{r}
ggseasonplot(power) +
  labs(colour = "Day")
```

The seasonality per day of the power consumption is interesting. We can see there is a strong season, there is a pick at around 8am, then a second one at around 7pm and it falls just before midnight. We can notice only one strange observation early in the morning of one of the last years.

```{r}
autoplot(power)
```

The outlayer on day ~48 with a value of 0 looks like a anomaly or missing value. This outlayer is on the end of the serie (testing set) and can distort our scoring. We can treat it as a missing value and predict its real value

## Replace outlayer

Let's spot the outlayer position

```{r}
zero_idx <- which(power == 0)
length(zero_idx)
```

```{r}
zero_times <- cbind(zero_idx %/% 96, zero_idx %% 96)
zero_times
```

We actually have 11 outlayers (2h45) and we have there timestamps, let's predict there values with a auto.arima model

```{r}
train <- window(power, end=zero_times[1])

outlayers_fit <- auto.arima(train, truncate = 96*10)
summary(outlayers_fit)
```

```{r}
pred <- forecast(outlayers_fit, h = 11)
autoplot(train) + autolayer(pred$mean)
```

Looks good, lets create our clean_power timeserie

```{r}
pred$mean
```


```{r}
clean_power <- power
#window(clean_power, start = start(pred$mean), end = end(pred$mean)) <- pred$mean

clean_power[zero_idx] <- pred$mean
autoplot(clean_power)

```

## Train / Test split + util functions

```{r}
train <- window(clean_power, end = c(48, 96))
test <- window(clean_power, start = c(49, 1))
```


```{r}
compute_RMSE <- function(y_pred, y_true, print=FALSE) {
  rmse <- sqrt(mean((y_true - y_pred)^2))
  if(print) {
    paste("RMSE: ", rmse)
  } else {
    return(rmse)
  }
}
```


## Hold Winters

```{r}
holt_fit <- holt(train, h=96)
```

```{r}
autoplot(train) + autolayer(holt_fit$mean)
```
Well obviously a simple holt is not good in this context. Let's try with a seasonal Holt Winter


## Holt Winters

We will use built in `HoltWinters` function instead as forecast's `hw` cause this last one doesn't work when frequency is higher than 24.

```{r}
hw_fit <- HoltWinters(train)
pred <- forecast(hw_fit, h = 96)
```

```{r}
autoplot(train) + autolayer(pred$mean)
```

Looks good, let's check the difference with the true values

```{r}
autoplot(pred$mean) + autolayer(test)
```
```{r}
compute_RMSE(pred$mean, test, print=T)
```

That's a good prediction. We will see how it compares with other methods.

## SARIMA

We had an easy time making a `auto.arima` to replace outlayers a the begenning of this exploration, but let's now manually find the appropriate SARIMA model for this time serie

```{r}
ggtsdisplay(diff(diff(train, lag=96), differences = 2))
```
We used 1 seasonal diff and 2 regular diff to get as close a possible to a white noise. We can see a nice exponential curve on PACF with a peak on the 96th lag. There is also what looks like an exponential curve on ACF with peaks on 1th, 2nd and 3rd season lags. Thereforce we can try a SARIMA(0, 2, 0)(3, 1, 1)

```{r}
arima_fit <- Arima(train, order = c(0, 2, 0), seasonal = c(0, 1, 1))
arima_fit %>% residuals() %>% ggtsdisplay()
```

```{r}
summary(auto_fit)
```

```{r}
arima_fit <- Arima(train, order = c(0, 2, 13), seasonal = c(0, 1, 1))
arima_fit %>% residuals() %>% ggtsdisplay()
```



There is now what looks like an exponential curve on the AFC too, let's add auto regressif of order 94

```{r}
train_light <- tail(train, 1000)
arima_fit <- Arima(train_light, order = c(0, 2, 0), seasonal = c(3, 1, 1))
arima_fit %>% residuals() %>% ggtsdisplay()
```

```{r}
xreg <- fourier(train, K = 3)

arima_fit <- Arima(train,
             order = c(0, 2, 0),
             seasonal = c(0, 1, 1),  # no seasonal AR/MA terms
             xreg = xreg)
arima_fit %>% residuals() %>% ggtsdisplay()
```



