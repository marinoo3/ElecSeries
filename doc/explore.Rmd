Imports

```{r}
library(fpp2)
library(ggplot2)
library(keras3)
library(tensorflow)
library(Elecseries)
```

Load data
```{r}
load("../data/power.rda")
load("../data/temp.rda")
```

## Explore the dataset

```{r}
autoplot(decompose(power, type='additive'))
```


```{r}
ggseasonplot(power) +
  labs(colour = "Day")
```

The seasonality per day of the power consumption is interesting. We can see there is a strong season, there is a pick at around 8am, then a second one at around 7pm and it falls just before midnight. We can notice only one strange observation early in the morning of one of the last years.

```{r}
autoplot(power)
```

The outlayer on day ~48 with a value of 0 looks like a anomaly or missing value. This outlayer is on the end of the serie (testing set) and can distort our scoring. We can treat it as a missing value and predict its real value

## Replace outlayer

Let's spot the outlayer position

```{r}
zero_idx <- which(power == 0)
length(zero_idx)
```

```{r}
zero_times <- cbind(zero_idx %/% 96, zero_idx %% 96)
zero_times
```

We actually have 11 outlayers (2h45) and we have there timestamps, let's predict there values with a auto.arima model

```{r}
train <- window(power, end=zero_times[1])

outlayers_fit <- auto.arima(train, truncate = 96*10)
summary(outlayers_fit)
```

```{r}
pred <- forecast(outlayers_fit, h = 11)
autoplot(train) + autolayer(pred$mean)
```

Looks good, lets create our clean_power timeserie

```{r}
pred$mean
```


```{r}
clean_power <- power
#window(clean_power, start = start(pred$mean), end = end(pred$mean)) <- pred$mean

clean_power[zero_idx] <- pred$mean
autoplot(clean_power)

```

## Train / Test split + util functions

```{r}
train <- window(clean_power, end = c(48, 96))
test <- window(clean_power, start = c(49, 1))
```


```{r}
compute_RMSE <- function(y_pred, y_true, print=FALSE) {
  rmse <- sqrt(mean((y_true - y_pred)^2))
  if(print) {
    paste("RMSE: ", rmse)
  } else {
    return(rmse)
  }
}
```


## Hold Winters

```{r}
holt_fit <- holt(train, h=96)
```

```{r}
autoplot(train) + autolayer(holt_fit$mean)
```
Well obviously a simple holt is not good in this context. Let's try with a seasonal Holt Winter


## Holt Winters

We will use built in `HoltWinters` function instead as forecast's `hw` cause this last one doesn't work when frequency is higher than 24.

```{r}
hw_fit <- HoltWinters(train)
pred <- forecast(hw_fit, h = 96)
```

```{r}
autoplot(train) + autolayer(pred$mean)
```

Looks good, let's check the difference with the true values

```{r}
autoplot(pred$mean) + autolayer(test)
```
```{r}
compute_RMSE(pred$mean, test, print=T)
```

That's a good prediction. We will see how it compares with other methods.

## SARIMA

We had an easy time making a `auto.arima` to replace outlayers a the begenning of this exploration, but let's now manually find the appropriate SARIMA model for this time serie

```{r}
ggtsdisplay(diff(diff(train, lag=96), differences = 2))
```
We used 1 seasonal diff and 2 regular diff to get as close a possible to a white noise. We can see a nice exponential curve on PACF with a peak on the 96th lag. There is also what looks like an exponential curve on ACF with peaks on 1th, 2nd and 3rd season lags. Thereforce we can try a SARIMA(0, 2, 0)(3, 1, 1)

I'll use a subset of the train set (1000 observations) cause my computer can't handle fitting the model on the entire train data

```{r}
train_light <- tail(train, 1000)
arima_fit <- Arima(train_light, order = c(0, 2, 0), seasonal = c(3, 1, 1))
arima_fit %>% residuals() %>% ggtsdisplay()
```

```{r}
pred <- forecast(arima_fit, h = 96)
autoplot(pred$mean) + autolayer(test)
```

```{r}
compute_RMSE(pred$mean, test, print = T)
```

We can try to compare the model without season lags on auto regressive side

```{r}
arima_fit <- Arima(train, order = c(0, 2, 0), seasonal = c(0, 1, 1))
arima_fit %>% residuals() %>% ggtsdisplay()
```

```{r}
pred <- forecast(arima_fit, h = 96)
```

```{r}
autoplot(pred$mean) + autolayer(test)
```

```{r}
compute_RMSE(pred$mean, test, print = T)
```

It is much better. We've reach the limit of what I can do with SARIMA on this dataset. Our best model was this last ARIMA(0, 2, 0)(0, 1, 1), even tho the auto.arima did better with a ARIMA(4,0,0)(0,1,0)...

## NNAR

```{r}
nnar_fit <- nnetar(train)
```

```{r}
pred <- forecast(nnar_fit, h = 96)
autoplot(pred$mean) + autolayer(test)
```

```{r}
compute_RMSE(pred$mean, test, print = T)
```

Not as good a out current best Holt Winters

## RNN

Normalize the data

```{r}
m <- mean(train)
s <- sd(test)

train_data_norm <- (train - m) / s
test_data_norm <- (test- m) / s

autoplot(train_data_norm, series="train")+
autolayer(test_data_norm, series="test")
```

`create_sequence` function. To build sequence of data

```{r}
create_sequences <- function(data, window_size) {
  X <- list()
  Y <- list()
  for (i in 1:(length(data) - window_size)) {
    X[[i]] <- data[i:(i + window_size - 1)]
    Y[[i]] <- data[i + window_size]
  }
  X <- array(unlist(X), dim=c(length(X), window_size, 1))
  Y <- unlist(Y)
  return(list(X = X, Y = Y))
}
```

X and y data split

```{r}
train_sequences <- create_sequences(train_data_norm, window_size = 96)
test_sequences <- create_sequences(test_data_norm, window_size = 90)

X_train <- train_sequences$X
Y_train <- train_sequences$Y
X_test <- test_sequences$X
Y_test <- test_sequences$Y
```

Build and train model

```{r}
model <- keras_model_sequential() %>%
  layer_simple_rnn(units = 50, activation = 'tanh', input_shape = c(96, 1)) %>%
  layer_dense(units = 1)

model %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'adam'
)

history <- model %>% fit(
  X_train, Y_train,
  epochs = 50,
  batch_size = 16,
  validation_data = list(X_test, Y_test)
)
```

Recursive forecasting function

```{r}
predict_recursive <- function(model, initial_sequence, n_ahead = 96, m, s) {
  preds <- numeric(n_ahead)
  input_seq <- initial_sequence
  
  for (i in 1:n_ahead) {
    input_array <- array(input_seq,
    dim = c(1,length(input_seq), 1))
    next_pred <- model %>% predict(input_array)
    preds[i] <- next_pred
    input_seq <- c(input_seq[-1],as.numeric(next_pred))
  }
  
  preds_denorm <- preds * s + m
  
  return(preds_denorm)
}
```

```{r}
initial_seq <- as.numeric(tail(train_data_norm, 96))
y_pred <- predict_recursive(model, initial_seq, n_ahead = 96, m, s)
```

```{r}
pred = ts(y_pred, start = start(test), end = end(test), frequency = 96)
```

```{r}
autoplot(train) + autolayer(pred)
```

```{r}
compute_RMSE(pred, test, print = T)
```

## WNN

Let's finally try on our own implementation of WNN

```{r}

```

